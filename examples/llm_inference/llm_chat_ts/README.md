# MediaPipe LLM Inference Chat Demo

This is a web-based chat demo that uses the MediaPipe LLM Inference API to run a large language model entirely in the browser.

## Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/google-ai-edge/mediapipe-samples.git
    ```
2.  **Navigate to the demo directory:**
    ```bash
    cd mediapipe-samples/examples/llm_inference/llm_chat_ts
    ```
3.  **Install the dependencies:**
    ```bash
    npm install
    ```

## Running the Demo

1.  **Run the development server:**
    ```bash
    npm run dev
    ```
    This will start a local development server and open the demo in your browser (latest Chrome is recommended). Any models you wish to try should be downloaded in advance from the litert-community "Web Models" collection on HuggingFace, and saved to mediapipe-samples/examples/llm_inference/llm_chat_ts/models/.
